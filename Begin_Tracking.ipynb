{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "from __future__ import division\n",
    "from scipy.misc import imsave\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import scipy as sc\n",
    "import nbimporter\n",
    "import tensorflow as tf\n",
    "from IPython.display import HTML \n",
    "import imp\n",
    "import time\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorboard\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/home/jeetkanjani7/Tonbo/siamfc-tf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow 1.5.0\n"
     ]
    }
   ],
   "source": [
    "from src.tracker import tracker\n",
    "#import src.siamese as siam\n",
    "import src.siamese as siam\n",
    "from src.parse_arguments import parse_arguments\n",
    "from src.region_to_bbox import region_to_bbox\n",
    "from src.visualization import show_frame, show_crops, show_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1 ,'/home/jeetkanjani7/Tonbo/siamfc-tf/Notebook/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import Siamese_nt as siam_nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_video():\n",
    "    video_folder = \"/home/jeetkanjani7/Tonbo/siamfc-tf/data/dataset/tc_Messi_ce/\"\n",
    "    frame_name_list = [f for f in os.listdir(video_folder) if f.endswith(\".jpg\")]\n",
    "    frame_name_list = [ (video_folder) + s for s in frame_name_list] \n",
    "    frame_name_list.sort()\n",
    "    with Image.open(frame_name_list[0]) as img:\n",
    "        frame_sz = np.asanyarray(img.size)\n",
    "        frame_sz[1], frame_sz[0] = frame_sz[0], frame_sz[1]\n",
    "        \n",
    "    gt_file = os.path.join(video_folder, 'groundtruth.txt')\n",
    "    gt = np.genfromtxt(gt_file, delimiter = ',')\n",
    "    n_frames = len(frame_name_list)\n",
    "    assert n_frames == len(gt), 'Number of frames and number of GT lines should be equal'\n",
    "    return gt, frame_name_list, frame_sz, n_frames \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
    "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
    "    \n",
    "    \n",
    "    if xA < xB and yA < yB:\n",
    "        interarea = (xB - xA) * (yB - yA)\n",
    "        \n",
    "        boxAArea = boxA[2] * boxA[3]\n",
    "        boxBArea = boxB[2] * boxB[3]\n",
    "        \n",
    "        iou = interarea/(boxAArea + boxBArea - interarea)\n",
    "    else:\n",
    "        iou = 0\n",
    "    assert iou >= 0\n",
    "    assert iou<=1.01\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_distance(boxA, boxB):\n",
    "    a = np.array((boxA[0]+boxA[2]/2, boxA[1]+boxA[3]/2))\n",
    "    b = np.array((boxB[0]+boxB[2]/2, boxB[1]+boxB[3]/2))\n",
    "    dist = np.linalg.norm(a - b)\n",
    "\n",
    "    assert dist >= 0\n",
    "    assert dist != float('Inf')\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_results(gt, bboxes, dist_threshold):\n",
    "    l = np.size(bboxes, 0)\n",
    "    gt4 = np.zeros((l,4))\n",
    "    new_distances = np.zeros(l)\n",
    "    new_ious = np.zeros(l)\n",
    "    n_thresholds = 50\n",
    "    precisions_ths = np.zeros(n_thresholds)\n",
    "    for i in range(l):\n",
    "        gt4[i, :] = region_to_bbox(gt[i, :], center=False)\n",
    "        new_distances[i] = _compute_distance(bboxes[i, :], gt4[i, :])\n",
    "        new_ious[i] = _compute_iou(bboxes[i, :], gt4[i, :])\n",
    "    precision = sum(new_distances < dist_threshold)/np.size(new_distances) * 100\n",
    "\n",
    "    thresholds = np.linspace(0, 25, n_thresholds+1)\n",
    "    thresholds = thresholds[-n_thresholds:]\n",
    "    thresholds = thresholds[::-1]\n",
    "    for i in range(n_thresholds):\n",
    "        precisions_ths[i] = sum(new_distances < thresholds[i])/np.size(new_distances)\n",
    "\n",
    "    precision_auc = np.trapz(precisions_ths)    \n",
    "    iou = np.mean(new_ious) * 100\n",
    "\n",
    "    return l, precision, precision_auc, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_video(env, evaluation, video):\n",
    "    video_folder = os.path.join(env.root_dataset, evaluation.dataset, video)\n",
    "    frame_name_list = [f for f in os.listdir(video_folder) if f.endswith(\".jpg\")]\n",
    "    frame_name_list = [os.path.join(env.root_dataset, evaluation.dataset, video, '') + s for s in frame_name_list]\n",
    "    frame_name_list.sort()\n",
    "    with Image.open(frame_name_list[0]) as img:\n",
    "        frame_sz = np.asarray(img.size)\n",
    "        frame_sz[1], frame_sz[0] = frame_sz[0], frame_sz[1]\n",
    "\n",
    "    # read the initialization from ground truth\n",
    "    gt_file = os.path.join(video_folder, 'groundtruth.txt')\n",
    "    gt = np.genfromtxt(gt_file, delimiter=',')\n",
    "    n_frames = len(frame_name_list)\n",
    "    assert n_frames == len(gt), 'Number of frames and number of GT lines should be equal.'\n",
    "\n",
    "    return gt, frame_name_list, frame_sz, n_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "[u'br_conv1f', u'br_conv1b', u'br_bn1m', u'br_bn1b', u'br_bn1x', u'br_conv2f', u'br_conv2b', u'br_bn2m', u'br_bn2b', u'br_bn2x', u'br_conv3f', u'br_conv3b', u'br_bn3m', u'br_bn3b', u'br_bn3x', u'br_conv4f', u'br_conv4b', u'br_bn4m', u'br_bn4b', u'br_bn4x', u'br_conv5f', u'br_conv5b', u'fin_adjust_bnm', u'fin_adjust_bnb', u'fin_adjust_bnx']\n",
      "> Layer 1\n",
      "\t\tCONV: setting br_conv1f br_conv1b\n",
      "\t\tCONV: stride 2, filter-group False\n",
      "\t\tBNORM: setting br_bn1b br_bn1m br_bn1x\n",
      "\t\tMAX-POOL: size 3 and stride 2\n",
      "> Layer 2\n",
      "\t\tCONV: setting br_conv2f br_conv2b\n",
      "\t\tCONV: stride 1, filter-group True\n",
      "\t\tBNORM: setting br_bn2b br_bn2m br_bn2x\n",
      "\t\tMAX-POOL: size 3 and stride 1\n",
      "> Layer 3\n",
      "\t\tCONV: setting br_conv3f br_conv3b\n",
      "\t\tCONV: stride 1, filter-group False\n",
      "\t\tBNORM: setting br_bn3b br_bn3m br_bn3x\n",
      "> Layer 4\n",
      "\t\tCONV: setting br_conv4f br_conv4b\n",
      "\t\tCONV: stride 1, filter-group True\n",
      "\t\tBNORM: setting br_bn4b br_bn4m br_bn4x\n",
      "> Layer 5\n",
      "\t\tCONV: setting br_conv5f br_conv5b\n",
      "\t\tCONV: stride 1, filter-group True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hp, evaluation, run, env, design = parse_arguments()\n",
    "final_score_sz = hp.response_up * (design.score_sz -1) + 1\n",
    "filename, image, templates_z, scores = siam_nt.build_tracking_graph_nt(final_score_sz, design, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt, frame_name_list, _ , _ = init_video(env, evaluation, evaluation.video)\n",
    "pos_x, pos_y, target_w, target_h = region_to_bbox(gt[evaluation.start_frame])\n",
    "final_score_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_w: 131.0\n",
      "150.42855447022018\n",
      "302.04158574729246\n",
      "752.1427723511009\n",
      "1510.2079287364622\n",
      "30.085710894044038\n",
      "60.4083171494585\n"
     ]
    }
   ],
   "source": [
    "num_frames = np.size(frame_name_list)\n",
    "# stores tracker's output for evaluation\n",
    "bboxes = np.zeros((num_frames,4))\n",
    "\n",
    "scale_factors = hp.scale_step**np.linspace(-np.ceil(hp.scale_num/2), np.ceil(hp.scale_num/2), hp.scale_num)\n",
    "# cosine window to penalize large displacements    \n",
    "hann_1d = np.expand_dims(np.hanning(final_score_sz), axis=0)\n",
    "penalty = np.transpose(hann_1d) * hann_1d\n",
    "penalty = penalty / np.sum(penalty)\n",
    "print('target_w: ' + str(target_w))\n",
    "context = design.context*(target_w+target_h)\n",
    "z_sz = np.sqrt(np.prod((target_w+context)*(target_h+context)))\n",
    "print(z_sz)\n",
    "x_sz = float(design.search_sz) / design.exemplar_sz * z_sz\n",
    "print (x_sz)\n",
    "# thresholds to saturate patches shrinking/growing\n",
    "\n",
    "min_z = hp.scale_min * z_sz\n",
    "max_z = hp.scale_max * z_sz\n",
    "min_x = hp.scale_min * x_sz\n",
    "max_x = hp.scale_max * x_sz\n",
    "print(max_z)\n",
    "print(max_x)\n",
    "print(min_z)\n",
    "print(min_x)\n",
    "\n",
    "run_opts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_position(pos_x, pos_y, score, final_score_sz, tot_stride, search_sz, response_up, x_sz):\n",
    "    # find location of score maximizer\n",
    "    p = np.asarray(np.unravel_index(np.argmax(score), np.shape(score)))\n",
    "    \n",
    "    # displacement from the center in search area final representation ...\n",
    "    center = float(final_score_sz - 1) / 2\n",
    "    disp_in_area = p - center\n",
    "    \n",
    "    # displacement from the center in instance crop\n",
    "    disp_in_xcrop = disp_in_area * float(tot_stride) / response_up\n",
    "    \n",
    "    # displacement from the center in instance crop (in frame coordinates)\n",
    "    disp_in_frame = disp_in_xcrop *  x_sz / search_sz\n",
    "    \n",
    "    # *position* within frame in frame coordinates\n",
    "    pos_y, pos_x = pos_y + disp_in_frame[0], pos_x + disp_in_frame[1]\n",
    "    return pos_x, pos_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Coordinate the loading of image files.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    # save first frame position (from ground-truth)\n",
    "    bboxes[0,:] = pos_x-target_w/2, pos_y-target_h/2, target_w, target_h                \n",
    "    \n",
    "    image_, templates_z_ = sess.run([image, templates_z], feed_dict={siam_nt.pos_x_ph: pos_x,\\\n",
    "                                                                     siam_nt.pos_y_ph: pos_y,\\\n",
    "                                                                     siam_nt.z_sz_ph: z_sz,\\\n",
    "                                                                    filename: frame_name_list[0]})\n",
    "    \"\"\"                \n",
    "    new_templates_z_ = templates_z_\n",
    "    \n",
    "    t_start = time.time()\n",
    "    \n",
    "    # Get an image from the queue\n",
    "    for i in range(1, int(num_frames/8)):        \n",
    "        \n",
    "        scaled_exemplar = z_sz * scale_factors\n",
    "        scaled_search_area = x_sz * scale_factors\n",
    "        scaled_target_w = target_w * scale_factors\n",
    "        scaled_target_h = target_h * scale_factors\n",
    "        \n",
    "    \n",
    "        image_, scores_ = sess.run(\n",
    "            [image, scores],\n",
    "            feed_dict={\n",
    "                siam_nt.pos_x_ph: pos_x,\n",
    "                siam_nt.pos_y_ph: pos_y,\n",
    "                siam_nt.x_sz0_ph: scaled_search_area[0],\n",
    "        \n",
    "                siam_nt.x_sz1_ph: scaled_search_area[1],\n",
    "                \n",
    "                siam_nt.x_sz2_ph: scaled_search_area[2],\n",
    "                templates_z: np.squeeze(templates_z_),\n",
    "                    filename: frame_name_list[i],\n",
    "                }, **run_opts)\n",
    "\n",
    "        scores_ = np.squeeze(scores_)\n",
    "    \n",
    "        # penalize change of scale\n",
    "        scores_[0,:,:] = hp.scale_penalty*scores_[0,:,:]\n",
    "        scores_[2,:,:] = hp.scale_penalty*scores_[2,:,:]\n",
    "        # find scale with highest peak (after penalty)\n",
    "        new_scale_id = np.argmax(np.amax(scores_, axis=(1,2)))\n",
    "        #print('Scale ID: ' + str(new_scale_id))\n",
    "        # update scaled sizes\n",
    "        \n",
    "        print('scaled search: '+ str(scaled_search_area[new_scale_id]))\n",
    "        x_sz = (1 - hp.scale_lr)*x_sz + hp.scale_lr * scaled_search_area[new_scale_id]   \n",
    "        if(x_sz < min_x): x_sz = min_x\n",
    "        print ('X_size: ' + str(x_sz))\n",
    "        \n",
    "     \n",
    "        target_w = (1-hp.scale_lr)*target_w + hp.scale_lr*scaled_target_w[new_scale_id]\n",
    "        target_h = (1-hp.scale_lr)*target_h + hp.scale_lr*scaled_target_h[new_scale_id]\n",
    "       \n",
    "        # select response with new_scale_id\n",
    "        score_ = scores_[new_scale_id,:,:]\n",
    "        score_ = score_ - np.min(score_)\n",
    "        score_ = score_/np.sum(score_)\n",
    "        # apply displacement penalty\n",
    "        score_ = (1-hp.window_influence)*score_ + hp.window_influence*penalty\n",
    "        #imageio.imwrite(\"./scores/\"+str(i)+\".jpg\", score_)\n",
    "        pos_x, pos_y = update_target_position(pos_x, pos_y, score_, final_score_sz, design.tot_stride, design.search_sz, hp.response_up, x_sz)\n",
    "        # convert <cx,cy,w,h> to <x,y,w,h> and save output\n",
    "        bboxes[i,:] = pos_x - target_w/2, pos_y-target_h/2, target_w, target_h\n",
    "        # update the target representation with a rolling average\n",
    "        if hp.z_lr>0:\n",
    "            new_templates_z_ = sess.run([templates_z], feed_dict={\n",
    "                                                                siam_nt.pos_x_ph: pos_x,\n",
    "                                                                siam_nt.pos_y_ph: pos_y,\n",
    "                                                                siam_nt.z_sz_ph: z_sz,\n",
    "                                                                image: image_\n",
    "                                                               })\n",
    "        templates_z_=(1-hp.z_lr)*np.asarray(templates_z_) + hp.z_lr*np.asarray(new_templates_z_)\n",
    "            \n",
    "            #update template patch size\n",
    "        z_sz = (1-hp.scale_lr)*z_sz + hp.scale_lr*scaled_exemplar[new_scale_id]\n",
    "            \n",
    "        if run.visualization:\n",
    "            show_frame(image_, bboxes[i,:], 1)  \n",
    "       \n",
    "    t_elapsed = time.time() - t_start\n",
    "    speed = num_frames/t_elapsed\n",
    "    coord.request_stop()\n",
    "    coord.join(threads) \n",
    "    plt.close('all')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "bboxes[0][0]\n",
    "img = cv2.imread('/home/jeetkanjani7/Tonbo/data/airplane/frame0.jpg', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(bboxes[0][0])\n",
    "cv2.rectangle(image_, (int(bboxes[0][0]),int(bboxes[0][1])),(int(bboxes[0][2]), int(bboxes[0][3])),(0,255,0),2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"/tmp/siamese/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_num = 3\n",
    "scale_step = 1.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factors = scale_step**np.linspace(-np.ceil(hp.scale_num/2), np.ceil(hp.scale_num/2), hp.scale_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index([4,5,6,7,8], (3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    image_, scores_ = sess.run(\n",
    "            [image, scores],\n",
    "            feed_dict={\n",
    "                siam_nt.pos_x_ph: pos_x,\n",
    "                siam_nt.pos_y_ph: pos_y,\n",
    "                siam_nt.x_sz0_ph: scaled_search_area[0],\n",
    "        \n",
    "                siam_nt.x_sz1_ph: scaled_search_area[1],\n",
    "                \n",
    "                siam_nt.x_sz2_ph: scaled_search_area[2],\n",
    "                templates_z: np.squeeze(templates_z_),\n",
    "                    filename: frame_name_list[i],\n",
    "                }, **run_opts)\n",
    "    temp = sess.run([templates_z], feed_dict={ siam_nt.pos_x_ph: pos_x,\n",
    "                                            siam_nt.pos_y_ph: pos_y,\n",
    "                                            siam_nt.z_sz_ph: z_sz,\n",
    "                                            image: image_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_search_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
